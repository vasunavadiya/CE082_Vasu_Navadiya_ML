{"nbformat":4,"nbformat_minor":0,"metadata":{"interpreter":{"hash":"6cd9a464138fc6ccabc335c6117ac38c085d16cbe40298286e63ec121b3c4a6a"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"LogisticRegression_TF.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"XqO28DmmjlBh"},"source":["## Replace Manual version of Logistic Regression with TF based version."]},{"cell_type":"code","metadata":{"id":"cvIAu0V-jlBm"},"source":["import nltk\n","from nltk.corpus import twitter_samples\n","import pandas as pd\n","import tensorflow as tf\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZi8UBQujlBq","executionInfo":{"status":"ok","timestamp":1633669224874,"user_tz":-330,"elapsed":1126,"user":{"displayName":"Vasu Navadiya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj49Kf7eTkTa95eu71x8L_s96mzeJkRu5ynsJEUrw=s64","userId":"13182116358444184138"}},"outputId":"e5bb90aa-d5d5-432f-b9e9-4036f79883f7"},"source":["nltk.download('twitter_samples')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"D7jFPUEgjlBs"},"source":["import re\n","import string\n","import numpy as np\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lcanN5pUjlBt"},"source":["#process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n","def process_tweet(tweet):\n","    \"\"\"Process tweet function.\n","    Input:\n","        tweet: a string containing a tweet\n","    Output:\n","        tweets_clean: a list of words containing the processed tweet\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","\n","    # remove stock market tickers like $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    # remove old style retweet text \"RT\"\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    # remove hyperlinks\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokenize tweets\n","\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        # 1 remove stopwords\n","        if word in stopwords_english:\n","            continue\n","        # 2 remove punctuation\n","        if word in string.punctuation:\n","            continue\n","        # 3 stemming word\n","        word = stemmer.stem(word)\n","        # 4 Add it to tweets_clean\n","        tweets_clean.append(word)\n","\n","    return tweets_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eCtqLwtnjlBu"},"source":["#build_freqs counts how often a word in the 'corpus' (the entire set of tweets) was associated with\n","# a positive label '1'         or\n","# a negative label '0',\n","\n","#then builds the freqs dictionary, where each key is a (word,label) tuple,\n","\n","#and the value is the count of its frequency within the corpus of tweets.\n","\n","def build_freqs(tweets, ys):\n","    \"\"\"Build frequencies.\n","    Input:\n","        tweets: a list of tweets\n","        ys: an m x 1 array with the sentiment label of each tweet\n","            (either 0 or 1)\n","    Output:\n","        freqs: a dictionary mapping each (word, sentiment) pair to its\n","        frequency\n","    \"\"\"\n","    # Convert np array to list since zip needs an iterable.\n","    # The squeeze is necessary or the list ends up with one element.\n","    # Also note that this is just a NOP if ys is already a list.\n","    yslist = np.squeeze(ys).tolist()\n","\n","    # Start with an empty dictionary and populate it by looping over all tweets\n","    # and over all processed words in each tweet.\n","    freqs = {}\n","\n","    for y, tweet in zip(yslist, tweets):\n","        for word in process_tweet(tweet):\n","            pair = (word, y)\n","\n","            #############################################################\n","            #Update the count of pair if present, set it to 1 otherwise\n","            if pair in freqs:\n","                freqs[pair] += 1\n","            else:\n","                freqs[pair] = 1\n","\n","    return freqs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gdE-UIrjlBv"},"source":["# select the set of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PCAFrfBQjlBw"},"source":["# split the data into two pieces, one for training and one for testing\n","#############################################################\n","from sklearn.model_selection import train_test_split\n","\n","train_pos, test_pos = train_test_split(all_positive_tweets, test_size=0.2)\n","train_neg, test_neg = train_test_split(all_negative_tweets, test_size=0.2)\n","\n","train_x = train_pos + train_neg\n","test_x = test_pos + test_neg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5PpSrOicjlBx"},"source":["# combine positive and negative labels\n","train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n","test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3uYBcIbZjlBy","executionInfo":{"status":"ok","timestamp":1633669382633,"user_tz":-330,"elapsed":4016,"user":{"displayName":"Vasu Navadiya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj49Kf7eTkTa95eu71x8L_s96mzeJkRu5ynsJEUrw=s64","userId":"13182116358444184138"}},"outputId":"2e578fbc-ac8c-49e3-aea7-393e9932fddc"},"source":["# create frequency dictionary\n","#############################################################\n","freqs = build_freqs(train_x, train_y)\n","\n","# check the output\n","print(\"type(freqs) = \" + str(type(freqs)))\n","print(\"len(freqs) = \" + str(len(freqs.keys())))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["type(freqs) = <class 'dict'>\n","len(freqs) = 11294\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mrBu_8pojlBy","executionInfo":{"status":"ok","timestamp":1633669388334,"user_tz":-330,"elapsed":390,"user":{"displayName":"Vasu Navadiya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj49Kf7eTkTa95eu71x8L_s96mzeJkRu5ynsJEUrw=s64","userId":"13182116358444184138"}},"outputId":"bb67a7be-d020-4cfb-8f79-93a33b5e70d9"},"source":["# Example\n","print('This is an example of a positive tweet: \\n', train_x[0])\n","print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is an example of a positive tweet: \n"," If you are looking to find out more about @VirtualUmbrella give me a shout, sending out lots of emails today :) #fridayfun\n","\n","This is an example of the processed version of the tweet: \n"," ['look', 'find', 'give', 'shout', 'send', 'lot', 'email', 'today', ':)', 'fridayfun']\n"]}]},{"cell_type":"code","metadata":{"id":"HjgiZbY3jlBz"},"source":["def extract_features(tweet, freqs):\n","    '''\n","    Input:\n","        tweet: a list of words for one tweet\n","        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","    Output:\n","        x: a feature vector of dimension (1,3)\n","    '''\n","    # tokenizes, stems, and removes stopwords\n","    #############################################################\n","    output = []\n","    for word_l in tweet:\n","        word_l = process_tweet(word_l)\n","\n","        # 3 elements in the form of a 1 x 3 vector\n","        x = np.zeros((1, 3))\n","\n","        #bias term is set to 1\n","        x[0,0] = 1\n","\n","        # loop through each word in the list of words\n","        for word in word_l:\n","\n","            # increment the word count for the positive label 1\n","            x[0,1] += freqs.get((word, 1.0),0)\n","\n","            # increment the word count for the negative label 0\n","            x[0,2] += freqs.get((word, 0.0),0)\n","\n","\n","        assert(x.shape == (1, 3))\n","        output.append(x)\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZRQfhjW6jlBz"},"source":["final_model = tf.keras.models.Sequential([\n","    tf.keras.layers.Dense(2, activation=tf.nn.softmax)\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmF_AlJ0jlBz"},"source":["final_model.compile(optimizer='sgd',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pBVtirf9jlB0","executionInfo":{"status":"ok","timestamp":1633669403336,"user_tz":-330,"elapsed":6045,"user":{"displayName":"Vasu Navadiya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj49Kf7eTkTa95eu71x8L_s96mzeJkRu5ynsJEUrw=s64","userId":"13182116358444184138"}},"outputId":"1e9021a3-95aa-464c-a8d8-9393b9aefb54"},"source":["final_model.fit(tf.convert_to_tensor(extract_features(train_x, freqs)), train_y, epochs=5)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","250/250 [==============================] - 1s 963us/step - loss: 10.6468 - accuracy: 0.9894\n","Epoch 2/5\n","250/250 [==============================] - 0s 989us/step - loss: 6.4801 - accuracy: 0.9906\n","Epoch 3/5\n","250/250 [==============================] - 0s 1ms/step - loss: 37.4414 - accuracy: 0.9890\n","Epoch 4/5\n","250/250 [==============================] - 0s 986us/step - loss: 32.8550 - accuracy: 0.9925\n","Epoch 5/5\n","250/250 [==============================] - 0s 982us/step - loss: 26.9334 - accuracy: 0.9931\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7faa3da68610>"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X73jq74mjlB0","executionInfo":{"status":"ok","timestamp":1633669407970,"user_tz":-330,"elapsed":1515,"user":{"displayName":"Vasu Navadiya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj49Kf7eTkTa95eu71x8L_s96mzeJkRu5ynsJEUrw=s64","userId":"13182116358444184138"}},"outputId":"ef03bc85-5355-42db-cdbd-28befd9f69da"},"source":["final_model.evaluate(tf.convert_to_tensor(extract_features(test_x, freqs)), test_y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["63/63 [==============================] - 0s 1ms/step - loss: 12.0783 - accuracy: 0.9905\n"]},{"output_type":"execute_result","data":{"text/plain":["[12.078258514404297, 0.9904999732971191]"]},"metadata":{},"execution_count":26}]}]}